{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>756</td>\n",
       "      <td>2549</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>905</td>\n",
       "      <td>1988</td>\n",
       "      <td>2631</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1263</td>\n",
       "      <td>1716</td>\n",
       "      <td>2603</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1216</td>\n",
       "      <td>1786</td>\n",
       "      <td>2769</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1821</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1208</td>\n",
       "      <td>1212</td>\n",
       "      <td>1411</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0            842     0          2.2         0   1       0           7    0.6   \n",
       "1           1021     1          0.5         1   0       1          53    0.7   \n",
       "2            563     1          0.5         1   2       1          41    0.9   \n",
       "3            615     1          2.5         0   0       0          10    0.8   \n",
       "4           1821     1          1.2         0  13       1          44    0.6   \n",
       "\n",
       "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0        188        2  ...         20       756  2549     9     7         19   \n",
       "1        136        3  ...        905      1988  2631    17     3          7   \n",
       "2        145        5  ...       1263      1716  2603    11     2          9   \n",
       "3        131        6  ...       1216      1786  2769    16     8         11   \n",
       "4        141        2  ...       1208      1212  1411     8     2         15   \n",
       "\n",
       "   three_g  touch_screen  wifi  price_range  \n",
       "0        0             0     1            1  \n",
       "1        1             1     0            2  \n",
       "2        1             1     0            2  \n",
       "3        1             0     0            2  \n",
       "4        1             1     0            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"dataset/train.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of       battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  \\\n",
       "0               842     0          2.2         0   1       0           7   \n",
       "1              1021     1          0.5         1   0       1          53   \n",
       "2               563     1          0.5         1   2       1          41   \n",
       "3               615     1          2.5         0   0       0          10   \n",
       "4              1821     1          1.2         0  13       1          44   \n",
       "5              1859     0          0.5         1   3       0          22   \n",
       "6              1821     0          1.7         0   4       1          10   \n",
       "7              1954     0          0.5         1   0       0          24   \n",
       "8              1445     1          0.5         0   0       0          53   \n",
       "9               509     1          0.6         1   2       1           9   \n",
       "10              769     1          2.9         1   0       0           9   \n",
       "11             1520     1          2.2         0   5       1          33   \n",
       "12             1815     0          2.8         0   2       0          33   \n",
       "13              803     1          2.1         0   7       0          17   \n",
       "14             1866     0          0.5         0  13       1          52   \n",
       "15              775     0          1.0         0   3       0          46   \n",
       "16              838     0          0.5         0   1       1          13   \n",
       "17              595     0          0.9         1   7       1          23   \n",
       "18             1131     1          0.5         1  11       0          49   \n",
       "19              682     1          0.5         0   4       0          19   \n",
       "20              772     0          1.1         1  12       0          39   \n",
       "21             1709     1          2.1         0   1       0          13   \n",
       "22             1949     0          2.6         1   4       0          47   \n",
       "23             1602     1          2.8         1   4       1          38   \n",
       "24              503     0          1.2         1   5       1           8   \n",
       "25              961     1          1.4         1   0       1          57   \n",
       "26              519     1          1.6         1   7       1          51   \n",
       "27              956     0          0.5         0   1       1          41   \n",
       "28             1453     0          1.6         1  12       1          52   \n",
       "29              851     0          0.5         0   3       0          21   \n",
       "...             ...   ...          ...       ...  ..     ...         ...   \n",
       "1970           1913     1          1.8         0   0       0          29   \n",
       "1971            538     0          1.1         1   0       1          25   \n",
       "1972           1191     0          0.8         0   6       1          46   \n",
       "1973            816     0          3.0         1   2       0           9   \n",
       "1974            915     1          0.5         1   9       1          33   \n",
       "1975           1157     1          0.8         0   7       0          27   \n",
       "1976           1201     1          0.5         0   2       0          10   \n",
       "1977           1379     0          1.1         1   1       1          18   \n",
       "1978           1483     1          2.2         0   3       1          53   \n",
       "1979           1614     0          1.2         0   1       1           9   \n",
       "1980            930     1          1.0         1   4       1           4   \n",
       "1981           1454     0          2.6         0   8       0           6   \n",
       "1982           1784     0          1.6         0   4       0          41   \n",
       "1983           1262     0          1.8         1  12       0          34   \n",
       "1984            797     0          2.2         1   0       0          37   \n",
       "1985           1829     1          2.1         0   8       0          59   \n",
       "1986           1139     1          0.9         1   6       1          58   \n",
       "1987            618     1          1.0         0   9       1          13   \n",
       "1988           1547     1          2.9         0   2       0          57   \n",
       "1989            586     0          2.8         0   2       0          15   \n",
       "1990           1617     1          2.4         0   8       1          36   \n",
       "1991           1882     0          2.0         0  11       1          44   \n",
       "1992            674     1          2.9         1   1       0          21   \n",
       "1993           1467     1          0.5         0   0       0          18   \n",
       "1994            858     0          2.2         0   1       0          50   \n",
       "1995            794     1          0.5         1   0       1           2   \n",
       "1996           1965     1          2.6         1   0       0          39   \n",
       "1997           1911     0          0.9         1   1       1          36   \n",
       "1998           1512     0          0.9         0   4       1          46   \n",
       "1999            510     1          2.0         1   5       1          45   \n",
       "\n",
       "      m_dep  mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  \\\n",
       "0       0.6        188        2  ...         20       756  2549     9     7   \n",
       "1       0.7        136        3  ...        905      1988  2631    17     3   \n",
       "2       0.9        145        5  ...       1263      1716  2603    11     2   \n",
       "3       0.8        131        6  ...       1216      1786  2769    16     8   \n",
       "4       0.6        141        2  ...       1208      1212  1411     8     2   \n",
       "5       0.7        164        1  ...       1004      1654  1067    17     1   \n",
       "6       0.8        139        8  ...        381      1018  3220    13     8   \n",
       "7       0.8        187        4  ...        512      1149   700    16     3   \n",
       "8       0.7        174        7  ...        386       836  1099    17     1   \n",
       "9       0.1         93        5  ...       1137      1224   513    19    10   \n",
       "10      0.1        182        5  ...        248       874  3946     5     2   \n",
       "11      0.5        177        8  ...        151      1005  3826    14     9   \n",
       "12      0.6        159        4  ...        607       748  1482    18     0   \n",
       "13      1.0        198        4  ...        344      1440  2680     7     1   \n",
       "14      0.7        185        1  ...        356       563   373    14     9   \n",
       "15      0.7        159        2  ...        862      1864   568    17    15   \n",
       "16      0.1        196        8  ...        984      1850  3554    10     9   \n",
       "17      0.1        121        3  ...        441       810  3752    10     2   \n",
       "18      0.6        101        5  ...        658       878  1835    19    13   \n",
       "19      1.0        121        4  ...        902      1064  2337    11     1   \n",
       "20      0.8         81        7  ...       1314      1854  2819    17    15   \n",
       "21      1.0        156        2  ...        974      1385  3283    17     1   \n",
       "22      0.3        199        4  ...        407       822  1433    11     5   \n",
       "23      0.7        114        3  ...        466       788  1037     8     7   \n",
       "24      0.4        111        3  ...        201      1245  2583    11     0   \n",
       "25      0.6        114        8  ...        291      1434  2782    18     9   \n",
       "26      0.3        132        4  ...        550       645  3763    16     1   \n",
       "27      1.0        143        7  ...        511      1075  3286    17     8   \n",
       "28      0.3         96        2  ...        187      1311  2373    10     1   \n",
       "29      0.4        200        5  ...       1171      1263   478    12     7   \n",
       "...     ...        ...      ...  ...        ...       ...   ...   ...   ...   \n",
       "1970    0.6        111        5  ...        675       742  2023    17    13   \n",
       "1971    0.3        163        7  ...        455       537  2215     9     3   \n",
       "1972    0.8         89        6  ...         42       807   824    19    18   \n",
       "1973    0.1        117        1  ...       1196      1651  3851    10     3   \n",
       "1974    0.3        199        2  ...        503       986  2156     7     3   \n",
       "1975    0.1         88        8  ...       1694      1798  2885     8     4   \n",
       "1976    1.0         99        7  ...        306       558   495    15     6   \n",
       "1977    0.2        129        2  ...        838       885  2358    10     5   \n",
       "1978    0.7        169        5  ...        291       651  1744     6     3   \n",
       "1979    0.1        161        3  ...        173      1219  1832    15     8   \n",
       "1980    0.9        144        8  ...       1017      1289  2016    13    10   \n",
       "1981    0.4        199        3  ...        698      1018  1300    10     0   \n",
       "1982    0.4        164        6  ...        610      1437  2313    14     1   \n",
       "1983    0.1        149        5  ...        223       737  3248    13     3   \n",
       "1984    0.9        144        7  ...        206      1167  2216     9     5   \n",
       "1985    0.1         91        5  ...       1457      1919  3142    16     6   \n",
       "1986    0.5        161        2  ...        742       999  1850     9     4   \n",
       "1987    0.1         80        4  ...        591       724  1424    15    12   \n",
       "1988    0.4        114        1  ...        347       957  1620     9     2   \n",
       "1989    0.2         83        3  ...        241       854  2592    12     8   \n",
       "1990    0.8         85        1  ...        743      1426   296     5     3   \n",
       "1991    0.8        113        8  ...          4       743  3579    19     8   \n",
       "1992    0.2        198        3  ...        576      1809  1180     6     3   \n",
       "1993    0.6        122        5  ...        888      1099  3962    15    11   \n",
       "1994    0.1         84        1  ...        528      1416  3978    17    16   \n",
       "1995    0.8        106        6  ...       1222      1890   668    13     4   \n",
       "1996    0.2        187        4  ...        915      1965  2032    11    10   \n",
       "1997    0.7        108        8  ...        868      1632  3057     9     1   \n",
       "1998    0.1        145        5  ...        336       670   869    18    10   \n",
       "1999    0.9        168        6  ...        483       754  3919    19     4   \n",
       "\n",
       "      talk_time  three_g  touch_screen  wifi  price_range  \n",
       "0            19        0             0     1            1  \n",
       "1             7        1             1     0            2  \n",
       "2             9        1             1     0            2  \n",
       "3            11        1             0     0            2  \n",
       "4            15        1             1     0            1  \n",
       "5            10        1             0     0            1  \n",
       "6            18        1             0     1            3  \n",
       "7             5        1             1     1            0  \n",
       "8            20        1             0     0            0  \n",
       "9            12        1             0     0            0  \n",
       "10            7        0             0     0            3  \n",
       "11           13        1             1     1            3  \n",
       "12            2        1             0     0            1  \n",
       "13            4        1             0     1            2  \n",
       "14            3        1             0     1            0  \n",
       "15           11        1             1     1            0  \n",
       "16           19        1             0     1            3  \n",
       "17           18        1             1     0            3  \n",
       "18           16        1             1     0            1  \n",
       "19           18        0             1     1            1  \n",
       "20            3        1             1     0            3  \n",
       "21           15        1             0     0            3  \n",
       "22           20        0             0     1            1  \n",
       "23           20        1             0     0            0  \n",
       "24           12        1             0     0            1  \n",
       "25            7        1             1     1            2  \n",
       "26            4        1             0     1            3  \n",
       "27           12        1             1     0            3  \n",
       "28           10        1             1     1            2  \n",
       "29           10        1             0     1            0  \n",
       "...         ...      ...           ...   ...          ...  \n",
       "1970          8        1             1     0            2  \n",
       "1971         17        1             1     1            1  \n",
       "1972          7        1             0     0            0  \n",
       "1973         14        1             0     1            3  \n",
       "1974         13        1             1     0            1  \n",
       "1975          2        1             0     1            3  \n",
       "1976         14        1             1     1            0  \n",
       "1977         15        1             1     0            2  \n",
       "1978         10        1             0     0            1  \n",
       "1979         11        1             0     0            1  \n",
       "1980         16        1             1     1            1  \n",
       "1981          2        0             0     1            1  \n",
       "1982         11        0             1     0            2  \n",
       "1983          4        0             1     1            2  \n",
       "1984          6        1             0     0            1  \n",
       "1985          5        1             1     1            3  \n",
       "1986          8        1             0     0            1  \n",
       "1987          7        1             1     0            0  \n",
       "1988         19        0             1     1            1  \n",
       "1989          3        0             0     0            1  \n",
       "1990          7        1             0     0            0  \n",
       "1991         20        1             1     0            3  \n",
       "1992          4        1             1     1            0  \n",
       "1993          5        1             1     1            3  \n",
       "1994          3        1             1     0            3  \n",
       "1995         19        1             1     0            0  \n",
       "1996         16        1             1     1            2  \n",
       "1997          5        1             1     0            3  \n",
       "1998         19        1             1     1            0  \n",
       "1999          2        1             1     1            3  \n",
       "\n",
       "[2000 rows x 21 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.420e+02 0.000e+00 2.200e+00 ... 0.000e+00 0.000e+00 1.000e+00]\n",
      " [1.021e+03 1.000e+00 5.000e-01 ... 1.000e+00 1.000e+00 0.000e+00]\n",
      " [5.630e+02 1.000e+00 5.000e-01 ... 1.000e+00 1.000e+00 0.000e+00]\n",
      " ...\n",
      " [1.911e+03 0.000e+00 9.000e-01 ... 1.000e+00 1.000e+00 0.000e+00]\n",
      " [1.512e+03 0.000e+00 9.000e-01 ... 1.000e+00 1.000e+00 1.000e+00]\n",
      " [5.100e+02 1.000e+00 2.000e+00 ... 1.000e+00 1.000e+00 1.000e+00]]\n",
      "\n",
      "[[1]\n",
      " [2]\n",
      " [2]\n",
      " ...\n",
      " [3]\n",
      " [0]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "#convert pandas Dataframe to numpy array\n",
    "\n",
    "X = data.iloc[:,:20].values\n",
    "y = data.iloc[:,20:21].values\n",
    "\n",
    "\n",
    "\n",
    "print(X)\n",
    "print()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.90259726 -0.9900495   0.83077942 ... -1.78686097 -1.00601811\n",
      "   0.98609664]\n",
      " [-0.49513857  1.0100505  -1.2530642  ...  0.55964063  0.99401789\n",
      "  -1.01409939]\n",
      " [-1.5376865   1.0100505  -1.2530642  ...  0.55964063  0.99401789\n",
      "  -1.01409939]\n",
      " ...\n",
      " [ 1.53077336 -0.9900495  -0.76274805 ...  0.55964063  0.99401789\n",
      "  -1.01409939]\n",
      " [ 0.62252745 -0.9900495  -0.76274805 ...  0.55964063  0.99401789\n",
      "   0.98609664]\n",
      " [-1.65833069  1.0100505   0.58562134 ...  0.55964063  0.99401789\n",
      "   0.98609664]]\n",
      "\n",
      "[[1]\n",
      " [2]\n",
      " [2]\n",
      " ...\n",
      " [3]\n",
      " [0]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "print(X)\n",
    "print()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       2\n",
       "2       2\n",
       "3       2\n",
       "4       1\n",
       "5       1\n",
       "6       3\n",
       "7       0\n",
       "8       0\n",
       "9       0\n",
       "10      3\n",
       "11      3\n",
       "12      1\n",
       "13      2\n",
       "14      0\n",
       "15      0\n",
       "16      3\n",
       "17      3\n",
       "18      1\n",
       "19      1\n",
       "20      3\n",
       "21      3\n",
       "22      1\n",
       "23      0\n",
       "24      1\n",
       "25      2\n",
       "26      3\n",
       "27      3\n",
       "28      2\n",
       "29      0\n",
       "       ..\n",
       "1970    2\n",
       "1971    1\n",
       "1972    0\n",
       "1973    3\n",
       "1974    1\n",
       "1975    3\n",
       "1976    0\n",
       "1977    2\n",
       "1978    1\n",
       "1979    1\n",
       "1980    1\n",
       "1981    1\n",
       "1982    2\n",
       "1983    2\n",
       "1984    1\n",
       "1985    3\n",
       "1986    1\n",
       "1987    0\n",
       "1988    1\n",
       "1989    1\n",
       "1990    0\n",
       "1991    3\n",
       "1992    0\n",
       "1993    3\n",
       "1994    3\n",
       "1995    0\n",
       "1996    2\n",
       "1997    3\n",
       "1998    0\n",
       "1999    3\n",
       "Name: price_range, Length: 2000, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perform one hot encoding of output class\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "Y = data['price_range']\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashpot\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder = OneHotEncoder()\n",
    "y = encoder.fit_transform(y).toarray()\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test and train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Build the Neural Network..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import dependencies\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1800/1800 [==============================] - 0s 183us/step - loss: 1.4635 - accuracy: 0.2483\n",
      "Epoch 2/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 1.3621 - accuracy: 0.3294\n",
      "Epoch 3/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 1.3029 - accuracy: 0.3839\n",
      "Epoch 4/100\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 1.2461 - accuracy: 0.4444\n",
      "Epoch 5/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 1.1843 - accuracy: 0.4833\n",
      "Epoch 6/100\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 1.1161 - accuracy: 0.5250\n",
      "Epoch 7/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 1.0435 - accuracy: 0.5750\n",
      "Epoch 8/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.9703 - accuracy: 0.6139\n",
      "Epoch 9/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.9007 - accuracy: 0.6428\n",
      "Epoch 10/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.8378 - accuracy: 0.6628\n",
      "Epoch 11/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.7799 - accuracy: 0.6983\n",
      "Epoch 12/100\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.7276 - accuracy: 0.7289\n",
      "Epoch 13/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.6786 - accuracy: 0.7517\n",
      "Epoch 14/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.6328 - accuracy: 0.7844\n",
      "Epoch 15/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.5890 - accuracy: 0.8078\n",
      "Epoch 16/100\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.5494 - accuracy: 0.8267\n",
      "Epoch 17/100\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.5141 - accuracy: 0.8361\n",
      "Epoch 18/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.4811 - accuracy: 0.8494\n",
      "Epoch 19/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.4507 - accuracy: 0.8733\n",
      "Epoch 20/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.4225 - accuracy: 0.8828\n",
      "Epoch 21/100\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.3982 - accuracy: 0.8967\n",
      "Epoch 22/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.3747 - accuracy: 0.9000\n",
      "Epoch 23/100\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.3535 - accuracy: 0.9067\n",
      "Epoch 24/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.3337 - accuracy: 0.9094\n",
      "Epoch 25/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.3163 - accuracy: 0.9144\n",
      "Epoch 26/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.3014 - accuracy: 0.9150\n",
      "Epoch 27/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.2867 - accuracy: 0.9194\n",
      "Epoch 28/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.2734 - accuracy: 0.9172\n",
      "Epoch 29/100\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.2601 - accuracy: 0.9289\n",
      "Epoch 30/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.2485 - accuracy: 0.9333\n",
      "Epoch 31/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.2397 - accuracy: 0.9350\n",
      "Epoch 32/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.2293 - accuracy: 0.9322\n",
      "Epoch 33/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.2202 - accuracy: 0.9383\n",
      "Epoch 34/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.2113 - accuracy: 0.9389\n",
      "Epoch 35/100\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.2037 - accuracy: 0.9400\n",
      "Epoch 36/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.1968 - accuracy: 0.9439\n",
      "Epoch 37/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.1910 - accuracy: 0.9444\n",
      "Epoch 38/100\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.1829 - accuracy: 0.9444\n",
      "Epoch 39/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.1771 - accuracy: 0.9478\n",
      "Epoch 40/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.1719 - accuracy: 0.9483\n",
      "Epoch 41/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.1672 - accuracy: 0.9506\n",
      "Epoch 42/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.1614 - accuracy: 0.9489\n",
      "Epoch 43/100\n",
      "1800/1800 [==============================] - 0s 43us/step - loss: 0.1565 - accuracy: 0.9550\n",
      "Epoch 44/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.1507 - accuracy: 0.9578\n",
      "Epoch 45/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.1470 - accuracy: 0.9572\n",
      "Epoch 46/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.1429 - accuracy: 0.9561\n",
      "Epoch 47/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.1386 - accuracy: 0.9567\n",
      "Epoch 48/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.1347 - accuracy: 0.9594\n",
      "Epoch 49/100\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.1317 - accuracy: 0.9600\n",
      "Epoch 50/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.1294 - accuracy: 0.9611\n",
      "Epoch 51/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.1254 - accuracy: 0.9661\n",
      "Epoch 52/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.1211 - accuracy: 0.9672\n",
      "Epoch 53/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.1177 - accuracy: 0.9683\n",
      "Epoch 54/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.1153 - accuracy: 0.9683\n",
      "Epoch 55/100\n",
      "1800/1800 [==============================] - 0s 46us/step - loss: 0.1116 - accuracy: 0.9750\n",
      "Epoch 56/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.1098 - accuracy: 0.9717\n",
      "Epoch 57/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.1059 - accuracy: 0.9744\n",
      "Epoch 58/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.1036 - accuracy: 0.9750\n",
      "Epoch 59/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.1015 - accuracy: 0.9744\n",
      "Epoch 60/100\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.0993 - accuracy: 0.9767\n",
      "Epoch 61/100\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.0968 - accuracy: 0.9783\n",
      "Epoch 62/100\n",
      "1800/1800 [==============================] - 0s 46us/step - loss: 0.0950 - accuracy: 0.9772\n",
      "Epoch 63/100\n",
      "1800/1800 [==============================] - 0s 43us/step - loss: 0.0929 - accuracy: 0.9800\n",
      "Epoch 64/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.0915 - accuracy: 0.9806\n",
      "Epoch 65/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.0884 - accuracy: 0.9811\n",
      "Epoch 66/100\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.0867 - accuracy: 0.9822\n",
      "Epoch 67/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0850 - accuracy: 0.9833\n",
      "Epoch 68/100\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.0829 - accuracy: 0.9850\n",
      "Epoch 69/100\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.0808 - accuracy: 0.9844\n",
      "Epoch 70/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.0800 - accuracy: 0.9856\n",
      "Epoch 71/100\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.0798 - accuracy: 0.9839\n",
      "Epoch 72/100\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.0785 - accuracy: 0.9861\n",
      "Epoch 73/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0757 - accuracy: 0.9856\n",
      "Epoch 74/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.0746 - accuracy: 0.9872\n",
      "Epoch 75/100\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.0729 - accuracy: 0.9878\n",
      "Epoch 76/100\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.0708 - accuracy: 0.9889\n",
      "Epoch 77/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.0686 - accuracy: 0.9894\n",
      "Epoch 78/100\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.0672 - accuracy: 0.9894\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 0s 48us/step - loss: 0.0672 - accuracy: 0.9867\n",
      "Epoch 80/100\n",
      "1800/1800 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.98 - 0s 36us/step - loss: 0.0685 - accuracy: 0.9872\n",
      "Epoch 81/100\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.0645 - accuracy: 0.9906\n",
      "Epoch 82/100\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.0642 - accuracy: 0.9894\n",
      "Epoch 83/100\n",
      "1800/1800 [==============================] - 0s 21us/step - loss: 0.0622 - accuracy: 0.9900\n",
      "Epoch 84/100\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.0620 - accuracy: 0.9900\n",
      "Epoch 85/100\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.0605 - accuracy: 0.9900\n",
      "Epoch 86/100\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.0587 - accuracy: 0.9917\n",
      "Epoch 87/100\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.0571 - accuracy: 0.9911\n",
      "Epoch 88/100\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.0559 - accuracy: 0.9933\n",
      "Epoch 89/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.0547 - accuracy: 0.9933\n",
      "Epoch 90/100\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.0538 - accuracy: 0.9922\n",
      "Epoch 91/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.0528 - accuracy: 0.9939\n",
      "Epoch 92/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0531 - accuracy: 0.9928\n",
      "Epoch 93/100\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.0523 - accuracy: 0.9928\n",
      "Epoch 94/100\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.0513 - accuracy: 0.9933\n",
      "Epoch 95/100\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.0504 - accuracy: 0.9933\n",
      "Epoch 96/100\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.0483 - accuracy: 0.9967\n",
      "Epoch 97/100\n",
      "1800/1800 [==============================] - 0s 23us/step - loss: 0.0482 - accuracy: 0.9944\n",
      "Epoch 98/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.0464 - accuracy: 0.9950\n",
      "Epoch 99/100\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.0461 - accuracy: 0.9967\n",
      "Epoch 100/100\n",
      "1800/1800 [==============================] - 0s 22us/step - loss: 0.0449 - accuracy: 0.9950\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=20, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "result = model.fit(X_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 1, 1, 0, 2, 0, 3, 3, 3, 1, 3, 1, 2, 0, 2, 0, 1, 2, 1, 2, 3, 1, 3, 3, 0, 3, 0, 2, 1, 0, 3, 1, 1, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 1, 3, 3, 3, 3, 1, 2, 2, 3, 2, 1, 0, 3, 0, 0, 0, 2, 1, 0, 3, 0, 0, 1, 1, 0, 3, 3, 0, 3, 0, 3, 3, 2, 0, 3, 0, 0, 1, 0, 3, 0, 3, 3, 3, 1, 1, 1, 0, 0, 2, 3, 2, 0, 3, 2, 1, 2, 2, 2, 3, 3, 0, 3, 2, 0, 0, 3, 0, 2, 1, 2, 1, 2, 2, 1, 2, 1, 0, 3, 0, 2, 0, 3, 2, 0, 0, 2, 3, 3, 3, 1, 3, 0, 3, 2, 3, 1, 2, 0, 2, 0, 3, 2, 1, 0, 0, 2, 0, 2, 1, 2, 1, 3, 3, 0, 2, 1, 1, 1, 3, 3, 1, 3, 3, 0, 3, 3, 0, 2, 2, 1, 0, 0, 1, 2, 1, 1, 1, 0, 0, 3, 3, 1, 2, 1, 1, 3, 1, 0, 1, 3, 1]\n",
      "\n",
      "[2, 1, 2, 1, 1, 0, 2, 0, 3, 3, 3, 1, 2, 1, 2, 0, 2, 0, 2, 2, 1, 2, 3, 1, 3, 3, 0, 3, 0, 2, 1, 0, 3, 1, 1, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 1, 2, 2, 3, 2, 1, 0, 3, 0, 0, 0, 2, 1, 0, 3, 0, 0, 1, 1, 0, 3, 3, 0, 3, 0, 3, 3, 2, 0, 3, 0, 0, 1, 0, 3, 0, 3, 2, 3, 1, 1, 1, 0, 0, 2, 3, 2, 0, 3, 2, 1, 2, 2, 2, 3, 3, 0, 3, 2, 0, 0, 3, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 0, 3, 0, 2, 0, 3, 2, 0, 0, 2, 3, 3, 3, 1, 3, 0, 3, 3, 3, 1, 2, 0, 3, 0, 3, 2, 1, 0, 0, 2, 0, 2, 1, 2, 1, 3, 3, 0, 2, 1, 1, 1, 3, 3, 1, 3, 3, 0, 3, 3, 0, 2, 2, 1, 0, 0, 1, 2, 1, 2, 1, 0, 0, 2, 3, 1, 2, 1, 1, 3, 1, 0, 2, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Convert predictions back to labels\n",
    "\n",
    "pred = list()\n",
    "for i in range(len(y_pred)):\n",
    "    pred.append(np.argmax(y_pred[i]))\n",
    "    \n",
    "\n",
    "    \n",
    "#Converting one hot encoded test label to label\n",
    "test = list()\n",
    "for i in range(len(y_test)):\n",
    "    test.append(np.argmax(y_test[i]))\n",
    "    \n",
    "print(pred)\n",
    "print()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing both result shows the accuracy of our neural network\n",
    "\n",
    "Nevetheless, let's use accuracy score to test our model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 94.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "a = accuracy_score(pred, test)\n",
    "print('Accuracy =', a*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize training and validation losses and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(result.history['acc'])\n",
    "#plt.plot(result.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
